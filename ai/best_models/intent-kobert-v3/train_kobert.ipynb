{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install konlpy",
   "id": "5f8eac3e73f02181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class KoBERTIntentSlotModel(nn.Module):\n",
    "    def __init__(self, num_intents, num_slots):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        self.intent_classifier = nn.Linear(hidden_size, num_intents)\n",
    "        self.slot_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_slots)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        return intent_logits, slot_logits\n"
   ],
   "id": "f28682c2045059b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# 📊 데이터 로드 및 전처리\n",
    "def load_and_preprocess_data():\n",
    "    df = pd.read_csv(\"intent_slot_dataset_cleaned.csv\")\n",
    "\n",
    "    df[\"intent_list\"] = df[\"intent_list\"].apply(\n",
    "        lambda x: json.loads(x) if isinstance(x, str) and x.strip().startswith(\"[\") else (x if isinstance(x, list) else [])\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_to_bio_word_level(sentence, slot_json):\n",
    "    tokens = sentence.split()\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    for slot_name, slot_value in slot_json.items():\n",
    "        slot_values = slot_value if isinstance(slot_value, list) else [slot_value]\n",
    "        for val in slot_values:\n",
    "            if not isinstance(val, str):\n",
    "                continue\n",
    "            val_tokens = val.split()\n",
    "\n",
    "            # ✅ 수정: 모든 매칭을 찾아서 처리\n",
    "            i = 0\n",
    "            while i <= len(tokens) - len(val_tokens):\n",
    "                if tokens[i:i+len(val_tokens)] == val_tokens:\n",
    "                    # 이미 태깅된 부분이 아닐 때만 태깅\n",
    "                    if all(labels[i+k] == 'O' for k in range(len(val_tokens))):\n",
    "                        labels[i] = f'B-{slot_name}'\n",
    "                        for j in range(1, len(val_tokens)):\n",
    "                            labels[i + j] = f'I-{slot_name}'\n",
    "                    i += len(val_tokens)  # 매칭된 부분 다음부터 계속 찾기\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "def create_bio_dataset(df_combined):\n",
    "    bio_data = []\n",
    "    for idx, (_, row) in enumerate(df_combined.iterrows()):\n",
    "        sentence = str(row['question']).strip()\n",
    "        if not sentence or sentence.lower() == 'nan':\n",
    "            continue\n",
    "\n",
    "        raw_slots = row['slots']\n",
    "        try:\n",
    "            if isinstance(raw_slots, str):\n",
    "                if raw_slots.strip() in ['', '[]', '{}', 'nan']:\n",
    "                    slot_dict = {}\n",
    "                else:\n",
    "                    cleaned = raw_slots.replace('null','None').replace('true','True').replace('false','False')\n",
    "                    slot_dict = ast.literal_eval(cleaned)\n",
    "            elif isinstance(raw_slots, dict):\n",
    "                slot_dict = raw_slots\n",
    "            else:\n",
    "                slot_dict = {}\n",
    "\n",
    "            tokens, labels = convert_to_bio_word_level(sentence, slot_dict)\n",
    "            bio_data.append({\n",
    "                \"intent_list\": row.get(\"intent_list\", []),  # ✅ 여기만 유지\n",
    "                \"tokens\": tokens,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame(bio_data)\n",
    "\n",
    "def create_mappings(df_bio):\n",
    "      \"\"\"Intent 및 Slot 매핑 생성 - Multi-label 지원\"\"\"\n",
    "      df_bio['labels'] = df_bio['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "      # Multi-label intent 처리: 모든 개별 intent를 수집\n",
    "      all_intents = set()\n",
    "      for intent_list in df_bio['intent_list']:\n",
    "          if isinstance(intent_list, list):\n",
    "              all_intents.update(intent_list)\n",
    "          else:\n",
    "              all_intents.add(intent_list)\n",
    "\n",
    "      # Intent 매핑 (개별 intent 기준)\n",
    "      intent_labels = sorted(all_intents)\n",
    "      intent2idx = {label: i for i, label in enumerate(intent_labels)}\n",
    "      idx2intent = {i: label for label, i in intent2idx.items()}\n",
    "\n",
    "      # Slot 라벨 수집 및 B/I 확장\n",
    "      original_labels = set()\n",
    "      for label_list in df_bio['labels']:\n",
    "          for label in label_list:\n",
    "              original_labels.add(label)\n",
    "              if label.startswith('B-'):\n",
    "                  original_labels.add('I-' + label[2:])\n",
    "\n",
    "      # Slot 매핑\n",
    "      slot_labels = sorted(original_labels)\n",
    "      slot2idx = {label: i for i, label in enumerate(slot_labels)}\n",
    "      idx2slot = {i: label for label, i in slot2idx.items()}\n",
    "\n",
    "      return intent2idx, idx2intent, slot2idx, idx2slot\n",
    "\n",
    "def create_balanced_dataset(df_bio, target_intent_samples=1155, target_slot_samples=300):\n",
    "    \"\"\"데이터 밸런싱 - Multi-label (intent_list) 지원\"\"\"\n",
    "\n",
    "    # ✅ stratify/그룹핑용 임시 키: intent_list를 정렬 튜플로\n",
    "    intent_tuple_key = df_bio['intent_list'].apply(lambda x: tuple(sorted(x)) if isinstance(x, list) else (str(x),))\n",
    "\n",
    "    # ⬇️ intent 조합별로 업/다운샘플\n",
    "    balanced_intent_dfs = []\n",
    "    for combo in intent_tuple_key.unique():\n",
    "        mask = intent_tuple_key == combo\n",
    "        intent_df = df_bio[mask]\n",
    "\n",
    "        if len(intent_df) >= target_intent_samples:\n",
    "            sampled_df = resample(intent_df, replace=False, n_samples=target_intent_samples, random_state=42)\n",
    "        else:\n",
    "            sampled_df = resample(intent_df, replace=True,  n_samples=target_intent_samples, random_state=42)\n",
    "\n",
    "        balanced_intent_dfs.append(sampled_df)\n",
    "\n",
    "    df_balanced_intent = pd.concat(balanced_intent_dfs, ignore_index=True)\n",
    "\n",
    "    # ⬇️ 슬롯 라벨 분포 보정\n",
    "    slot_counter = Counter(label for labels in df_balanced_intent['labels'] for label in labels)\n",
    "    rare_slots = [label for label, cnt in slot_counter.items() if cnt < target_slot_samples]\n",
    "\n",
    "    slot_augmented_dfs = []\n",
    "    for rare_label in rare_slots:\n",
    "        slot_df = df_bio[df_bio['labels'].apply(lambda lst: rare_label in lst)]\n",
    "        if len(slot_df) == 0:\n",
    "            continue\n",
    "        needed = target_slot_samples - slot_counter[rare_label]\n",
    "        if needed > 0:\n",
    "            dup_df = resample(slot_df, replace=True, n_samples=needed, random_state=42)\n",
    "            slot_augmented_dfs.append(dup_df)\n",
    "\n",
    "    df_balanced = pd.concat([df_balanced_intent] + slot_augmented_dfs, ignore_index=True) if slot_augmented_dfs else df_balanced_intent\n",
    "\n",
    "    # ✅ 디버그용 출력(의도 조합 분포)\n",
    "    counts = intent_tuple_key.value_counts()\n",
    "    print(\"✅ Intent 조합 개수:\", len(counts))\n",
    "    print(\"상위 10개 조합:\")\n",
    "    for combo, cnt in counts.head(10).items():\n",
    "        print(f\"  {combo}: {cnt}\")\n",
    "\n",
    "    print(f\"\\n✅ 최종 데이터셋 크기: {len(df_balanced)}\")\n",
    "    return df_balanced\n",
    "\n",
    "# 🏷️ BCEWithLogitsLoss 지원 Dataset 클래스\n",
    "class IntentSlotDataset(Dataset):\n",
    "    def __init__(self, encodings, slot_labels, intents, intent2idx, use_bce=True):\n",
    "        self.encodings = encodings\n",
    "        self.slot_labels = slot_labels\n",
    "        self.intents = intents\n",
    "        self.intent2idx = intent2idx\n",
    "        self.use_bce = use_bce\n",
    "        self.num_intents = len(intent2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.intents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.slot_labels[idx])\n",
    "\n",
    "        # Multi-label intent를 one-hot vector로 변환\n",
    "        intent_vector = torch.zeros(self.num_intents)\n",
    "        intent_labels = self.intents[idx]\n",
    "\n",
    "        if isinstance(intent_labels, list):\n",
    "            for intent_idx in intent_labels:\n",
    "                intent_vector[intent_idx] = 1\n",
    "        else:\n",
    "            intent_vector[intent_labels] = 1\n",
    "\n",
    "        item['intent'] = intent_vector\n",
    "        return item\n",
    "\n",
    "def align_labels_with_tokenizer(tokens, labels, tokenizer):\n",
    "    \"\"\"토큰과 라벨 정렬\"\"\"\n",
    "    bert_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        if not sub_tokens:\n",
    "            sub_tokens = [tokenizer.unk_token]\n",
    "\n",
    "        bert_tokens.extend(sub_tokens)\n",
    "        aligned_labels.append(label)\n",
    "\n",
    "        for _ in range(1, len(sub_tokens)):\n",
    "            if label == \"O\":\n",
    "                aligned_labels.append(\"O\")\n",
    "            elif label.startswith(\"B-\"):\n",
    "                aligned_labels.append(\"I-\" + label[2:])\n",
    "            elif label.startswith(\"I-\"):\n",
    "                aligned_labels.append(label)\n",
    "            else:\n",
    "                aligned_labels.append(\"O\")\n",
    "\n",
    "    return bert_tokens, aligned_labels\n",
    "\n",
    "def _ensure_list(x):\n",
    "    # intent_list가 리스트/JSON/문자열 어떤 형태여도 리스트로 표준화\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith('[') and s.endswith(']'):\n",
    "            try:\n",
    "                arr = json.loads(s)\n",
    "                return [str(v).strip() for v in arr if str(v).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        return [t.strip() for t in s.split(',') if t.strip()]\n",
    "    return [] if pd.isna(x) else [str(x).strip()]\n",
    "\n",
    "def encode_data(df, tokenizer, intent2idx, slot2idx, max_len=64):\n",
    "    \"\"\"데이터 인코딩 - Multi-label (intent_list) 지원\"\"\"\n",
    "    input_ids, attention_masks, slot_label_ids, intent_ids = [], [], [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = row[\"tokens\"]\n",
    "        labels = row[\"labels\"]\n",
    "\n",
    "        # 토크나이저 정렬\n",
    "        bert_tokens, aligned_labels = align_labels_with_tokenizer(tokens, labels, tokenizer)\n",
    "        label_ids = [slot2idx[label] for label in aligned_labels]\n",
    "\n",
    "        # 길이 자르기\n",
    "        if len(bert_tokens) > max_len - 2:\n",
    "            bert_tokens = bert_tokens[:max_len - 2]\n",
    "            label_ids   = label_ids[:max_len - 2]\n",
    "\n",
    "        # [CLS], [SEP] / 패딩\n",
    "        tokens_input = ['[CLS]'] + bert_tokens + ['[SEP]']\n",
    "        label_ids    = [slot2idx['O']] + label_ids + [slot2idx['O']]\n",
    "\n",
    "        input_id       = tokenizer.convert_tokens_to_ids(tokens_input)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "\n",
    "        pad_len = max_len - len(input_id)\n",
    "        if pad_len > 0:\n",
    "            input_id       += [0] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            label_ids      += [slot2idx['O']] * pad_len\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        slot_label_ids.append(label_ids)\n",
    "\n",
    "        # ✅ Multi-label intent_list → index 리스트\n",
    "        intents = _ensure_list(row.get(\"intent_list\", []))\n",
    "        intent_indices = [intent2idx[i] for i in intents if i in intent2idx]\n",
    "\n",
    "        # 최소 1개는 보장(없으면 빈 리스트 유지해도 됨; Dataset에서 원핫 만들 때 0으로만 채워짐)\n",
    "        intent_ids.append(intent_indices)\n",
    "\n",
    "    encodings = {\"input_ids\": input_ids, \"attention_mask\": attention_masks}\n",
    "    return encodings, slot_label_ids, intent_ids\n",
    "\n",
    "def create_datasets(df_balanced, tokenizer, intent2idx, slot2idx, use_bce=True, max_len=64, test_size=0.1):\n",
    "      \"\"\"Dataset 및 DataLoader 생성 - Stratify 문제 해결\"\"\"\n",
    "\n",
    "      df_balanced['intent_tuple'] = df_balanced['intent_list'].apply(lambda x: tuple(sorted(x)))\n",
    "\n",
    "      # Train/Val 분할\n",
    "      train_df, val_df = train_test_split(\n",
    "          df_balanced,\n",
    "          test_size=test_size,\n",
    "          stratify=df_balanced['intent_tuple'],\n",
    "          random_state=42\n",
    "      )\n",
    "\n",
    "      # intent_str 컬럼 제거\n",
    "      train_df = train_df.drop(columns=['intent_tuple'])\n",
    "      val_df = val_df.drop(columns=['intent_tuple'])\n",
    "\n",
    "      # 인코딩\n",
    "      train_encodings, train_slot_labels, train_intents = encode_data(train_df, tokenizer, intent2idx, slot2idx,\n",
    "  max_len)\n",
    "      val_encodings, val_slot_labels, val_intents = encode_data(val_df, tokenizer, intent2idx, slot2idx, max_len)\n",
    "\n",
    "      # Dataset 생성\n",
    "      train_dataset = IntentSlotDataset(train_encodings, train_slot_labels, train_intents, intent2idx, use_bce)\n",
    "      val_dataset = IntentSlotDataset(val_encodings, val_slot_labels, val_intents, intent2idx, use_bce)\n",
    "\n",
    "      # DataLoader 생성\n",
    "      train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "      val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "      print(f\"✅ {'BCE' if use_bce else 'CrossEntropy'}용 데이터셋 생성 완료\")\n",
    "      print(f\"   Train: {len(train_dataset)} samples\")\n",
    "      print(f\"   Val: {len(val_dataset)} samples\")\n",
    "\n",
    "      return train_loader, val_loader\n",
    "\n",
    "# 🔧 학습 및 평가 함수\n",
    "def train_epoch_bce(model, dataloader, optimizer, device, intent2idx, slot2idx, slot_weights=None):\n",
    "    \"\"\"BCE 손실을 사용한 학습\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loss functions\n",
    "    intent_loss_fn = BCEWithLogitsLoss()\n",
    "    if slot_weights is not None:\n",
    "        slot_loss_fn = nn.CrossEntropyLoss(weight=slot_weights)\n",
    "    else:\n",
    "        slot_loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"🛠️ Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        slot_labels = batch['labels'].to(device)\n",
    "        intent_labels = batch['intent'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        intent_logits, slot_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Intent loss (BCE)\n",
    "        loss_intent = intent_loss_fn(intent_logits, intent_labels)\n",
    "\n",
    "        # Slot loss (CrossEntropy)\n",
    "        loss_slot = slot_loss_fn(slot_logits.view(-1, len(slot2idx)), slot_labels.view(-1))\n",
    "\n",
    "        loss = loss_intent + loss_slot\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_bce(model, dataloader, device, intent2idx, slot2idx, threshold=0.5):\n",
    "    \"\"\"BCE 기반 평가\"\"\"\n",
    "    model.eval()\n",
    "    intent_preds, intent_trues = [], []\n",
    "    slot_preds, slot_trues = [], []\n",
    "\n",
    "    idx2intent = {v: k for k, v in intent2idx.items()}\n",
    "    idx2slot = {v: k for k, v in slot2idx.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"🔍 Validating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            slot_labels = batch['labels'].to(device)\n",
    "            intent_labels = batch['intent'].to(device)\n",
    "\n",
    "            intent_logits, slot_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Intent 예측 (sigmoid + threshold)\n",
    "            intent_probs = torch.sigmoid(intent_logits)\n",
    "            intent_pred = (intent_probs > threshold).float()\n",
    "\n",
    "            intent_preds.extend(intent_pred.cpu().numpy())\n",
    "            intent_trues.extend(intent_labels.cpu().numpy())\n",
    "\n",
    "            # Slot 예측\n",
    "            slot_pred = torch.argmax(slot_logits, dim=2)\n",
    "            for i in range(slot_labels.size(0)):\n",
    "                true_seq = slot_labels[i].cpu().tolist()\n",
    "                pred_seq = slot_pred[i].cpu().tolist()\n",
    "                for t, p in zip(true_seq, pred_seq):\n",
    "                    if t != -100:\n",
    "                        slot_trues.append(t)\n",
    "                        slot_preds.append(p)\n",
    "\n",
    "    # Intent 정확도 계산\n",
    "    intent_preds = np.array(intent_preds)\n",
    "    intent_trues = np.array(intent_trues)\n",
    "\n",
    "    exact_match = np.all(intent_preds == intent_trues, axis=1)\n",
    "    intent_acc = np.mean(exact_match)\n",
    "\n",
    "    intent_report = classification_report(\n",
    "        intent_trues, intent_preds,\n",
    "        target_names=list(intent2idx.keys()),\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Slot 정확도 계산\n",
    "    support_counter = Counter(slot_trues)\n",
    "    nonzero_labels = [i for i in slot2idx.values() if support_counter[i] > 0]\n",
    "    target_names_nonzero = [key for key, val in slot2idx.items() if val in nonzero_labels]\n",
    "\n",
    "    slot_acc = accuracy_score(slot_trues, slot_preds)\n",
    "    slot_report = classification_report(\n",
    "        slot_trues, slot_preds,\n",
    "        labels=nonzero_labels,\n",
    "        target_names=target_names_nonzero,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    return intent_acc, intent_report, slot_acc, slot_report\n",
    "\n",
    "def train_with_bce(model, train_loader, val_loader, device, intent2idx, slot2idx,\n",
    "                   slot_weights=None, epochs=10, lr=5e-5, threshold=0.5, save_path=None):\n",
    "    \"\"\"BCEWithLogitsLoss로 학습\"\"\"\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # 결과 저장용\n",
    "    train_losses = []\n",
    "    val_intent_accuracies = []\n",
    "    val_slot_accuracies = []\n",
    "\n",
    "    best_val_intent_acc = 0\n",
    "    best_model_state = None\n",
    "    best_intent_report = \"\"\n",
    "    best_slot_report = \"\"\n",
    "\n",
    "    print(\"🚀 BCEWithLogitsLoss Training Started\")\n",
    "    print(f\"📊 Threshold: {threshold}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n📚 Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_epoch_bce(\n",
    "            model, train_loader, optimizer, device,\n",
    "            intent2idx, slot2idx, slot_weights\n",
    "        )\n",
    "\n",
    "        # 평가\n",
    "        val_intent_acc, intent_report, val_slot_acc, slot_report = evaluate_bce(\n",
    "            model, val_loader, device, intent2idx, slot2idx, threshold\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        train_losses.append(train_loss)\n",
    "        val_intent_accuracies.append(val_intent_acc)\n",
    "        val_slot_accuracies.append(val_slot_acc)\n",
    "\n",
    "        print(f\"📉 Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"🎯 Val Intent Accuracy: {val_intent_acc:.4f}\")\n",
    "        print(f\"🏷 Val Slot Accuracy: {val_slot_acc:.4f}\")\n",
    "\n",
    "        # Best model 저장\n",
    "        if val_intent_acc > best_val_intent_acc:\n",
    "            best_val_intent_acc = val_intent_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_intent_report = intent_report\n",
    "            best_slot_report = slot_report\n",
    "            print(\"✅ Best model updated!\")\n",
    "\n",
    "            # 모델 저장\n",
    "            if save_path:\n",
    "                import os\n",
    "                import pickle\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                torch.save(best_model_state, os.path.join(save_path, \"best_model.pt\"))\n",
    "                with open(os.path.join(save_path, \"intent2idx.pkl\"), \"wb\") as f:\n",
    "                    pickle.dump(intent2idx, f)\n",
    "                with open(os.path.join(save_path, \"slot2idx.pkl\"), \"wb\") as f:\n",
    "                    pickle.dump(slot2idx, f)\n",
    "\n",
    "    print(f\"\\n🎉 Training Completed!\")\n",
    "    print(f\"📈 Best Intent Accuracy: {best_val_intent_acc:.4f}\")\n",
    "    print(\"\\n📊 Best Intent Classification Report:\")\n",
    "    print(best_intent_report)\n",
    "    print(\"\\n📊 Best Slot Classification Report:\")\n",
    "    print(best_slot_report)\n",
    "\n",
    "    return {\n",
    "        'best_model_state': best_model_state,\n",
    "        'train_losses': train_losses,\n",
    "        'val_intent_accuracies': val_intent_accuracies,\n",
    "        'val_slot_accuracies': val_slot_accuracies,\n",
    "        'best_intent_acc': best_val_intent_acc,\n",
    "        'best_intent_report': best_intent_report,\n",
    "        'best_slot_report': best_slot_report\n",
    "    }\n",
    "\n",
    "def plot_training_curves(results, save_path=None):\n",
    "    \"\"\"학습 곡선 시각화\"\"\"\n",
    "    train_losses = results['train_losses']\n",
    "    val_intent_accuracies = results['val_intent_accuracies']\n",
    "    val_slot_accuracies = results['val_slot_accuracies']\n",
    "\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # 왼쪽 Y축: Train Loss\n",
    "    ax1.plot(epochs, train_losses, color='blue', label='Train Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # 오른쪽 Y축: Accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, val_intent_accuracies, color='orange', label='Intent Accuracy')\n",
    "    ax2.plot(epochs, val_slot_accuracies, color='green', label='Slot Accuracy')\n",
    "    ax2.set_ylabel('Accuracy', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    # 범례\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='lower center')\n",
    "\n",
    "    plt.title('📊 BCEWithLogitsLoss Training Results')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"📊 Plot saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 🚀 전체 파이프라인 실행 함수\n",
    "def run_bce_training_pipeline(model_class, save_path=\"best_models/intent-bce-v1\"):\n",
    "    \"\"\"전체 BCEWithLogitsLoss 학습 파이프라인\"\"\"\n",
    "\n",
    "    print(\"🔄 1단계: 데이터 로드 및 전처리...\")\n",
    "    df_combined = load_and_preprocess_data()\n",
    "    df_bio = create_bio_dataset(df_combined)\n",
    "\n",
    "    print(\"🔄 2단계: 매핑 생성...\")\n",
    "    intent2idx, idx2intent, slot2idx, idx2slot = create_mappings(df_bio)\n",
    "\n",
    "    print(\"🔄 3단계: 데이터 밸런싱...\")\n",
    "    df_balanced = create_balanced_dataset(df_bio)\n",
    "\n",
    "    print(\"🔄 4단계: 토크나이저 로드...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False)\n",
    "\n",
    "    print(\"🔄 5단계: 데이터셋 생성...\")\n",
    "    train_loader, val_loader = create_datasets(\n",
    "        df_balanced, tokenizer, intent2idx, slot2idx, use_bce=True\n",
    "    )\n",
    "\n",
    "    print(\"🔄 6단계: 모델 초기화...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class(len(intent2idx), len(slot2idx)).to(device)\n",
    "\n",
    "    print(\"🔄 7단계: 학습 시작...\")\n",
    "    results = train_with_bce(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        intent2idx=intent2idx,\n",
    "        slot2idx=slot2idx,\n",
    "        epochs=5,\n",
    "        threshold=0.5,\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "    print(\"🔄 8단계: 결과 시각화...\")\n",
    "    plot_training_curves(results, f\"{save_path}/training_curves.png\" if save_path else None)\n",
    "\n",
    "    return results, intent2idx, slot2idx\n",
    "\n",
    "# 💡 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 Unified BCEWithLogitsLoss Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results, intent2idx, slot2idx = run_bce_training_pipeline(KoBERTIntentSlotModel)\n",
    "\n",
    "    print(\"\"\"\n",
    "    사용법:\n",
    "    1. 모델 클래스 import: from your_model import KoBERTIntentSlotModel\n",
    "    2. 파이프라인 실행: results, intent2idx, slot2idx = run_bce_training_pipeline(KoBERTIntentSlotModel)\n",
    "\n",
    "    또는 단계별 실행:\n",
    "    1. df_combined = load_and_preprocess_data()\n",
    "    2. df_bio = create_bio_dataset(df_combined)\n",
    "    3. intent2idx, idx2intent, slot2idx, idx2slot = create_mappings(df_bio)\n",
    "    4. df_balanced = create_balanced_dataset(df_bio)\n",
    "    5. tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False)\n",
    "    6. train_loader, val_loader = create_datasets(df_balanced, tokenizer, intent2idx, slot2idx)\n",
    "    7. results = train_with_bce(model, train_loader, val_loader, device, intent2idx, slot2idx)\n",
    "    \"\"\")"
   ],
   "id": "e826fa523a058072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " AIRLINE_CODES = [\n",
    "    \"KE\", \"OZ\", \"7C\", \"LJ\", \"BX\", \"ZE\", \"TW\", \"RS\", \"YP\", \"RF\", \"KJ\", \"2I\", \"Q5\",\n",
    "    \"FE\", \"AQ\", \"Y6\", \"GB\", \"K4\", \"YJ\", \"GP\", \"3V\", \"5O\", \"PH\", \"4A\", \"CJ\", \"A0\",\n",
    "    \"B5\", \"8H\", \"H5\", \"2C\", \"OK\", \"DX\", \"L3\", \"ES\", \"D0\", \"Q7\", \"D5\", \"7T\", \"LI\",\n",
    "    \"8D\", \"C0\", \"9A\", \"IV\", \"A5\", \"II\", \"AZ\", \"XE\", \"FK\", \"KL\", \"WA\", \"KK\", \"LV\",\n",
    "    \"GE\", \"LO\", \"WW\", \"7M\", \"M2\", \"OM\", \"MB\", \"6V\", \"MU\", \"2P\", \"PB\", \"CG\", \"OH\",\n",
    "    \"S7\", \"RZ\", \"5S\", \"SK\", \"SL\", \"SP\", \"O3\", \"KI\", \"S0\", \"8F\", \"DT\", \"T0\", \"TP\",\n",
    "    \"TP\", \"YQ\", \"RO\", \"X3\", \"OR\", \"6B\", \"TB\", \"BY\", \"GO\", \"5X\", \"UD\", \"BS\", \"UJ\",\n",
    "    \"RT\", \"V6\", \"2Z\", \"YG\", \"GA\", \"GT\", \"G2\", \"IF\", \"GF\", \"GW\", \"G8\", \"JS\", \"G7\",\n",
    "    \"6G\", \"G3\", \"Y5\", \"G1\", \"5U\", \"GX\", \"CN\", \"YR\", \"GV\", \"HB\", \"Q9\", \"Z5\", \"G6\",\n",
    "    \"5S\", \"GE\", \"ON\", \"NP\", \"NJ\", \"T2\", \"NM\", \"IN\", \"SA\", \"N8\", \"9Y\", \"NA\", \"NE\",\n",
    "    \"NO\", \"RA\", \"1I\", \"NC\", \"Y7\", \"N4\", \"ND\", \"N7\", \"Z0\", \"N0\", \"DY\", \"DY\", \"D8\",\n",
    "    \"NA\", \"O9\", \"NG\", \"VQ\", \"0N\", \"HW\", \"J3\", \"DD\", \"N5\", \"BJ\", \"EJ\", \"7H\", \"OJ\",\n",
    "    \"9J\", \"DN\", \"D3\", \"DL\", \"R6\", \"ZQ\", \"DZ\", \"KB\", \"3R\", \"4Y\", \"DF\", \"LC\", \"B0\",\n",
    "    \"R4\", \"TH\", \"LK\", \"QV\", \"LW\", \"7S\", \"FR\", \"RK\", \"JT\", \"8V\", \"M3\", \"UC\", \"L7\",\n",
    "    \"LA\", \"JJ\", \"XL\", \"LA\", \"4C\", \"PZ\", \"LP\", \"LQ\", \"TM\", \"P7\", \"BN\", \"8L\", \"WZ\",\n",
    "    \"L5\", \"7H\", \"QL\", \"R0\", \"LC\", \"ZL\", \"LM\", \"FV\", \"AT\", \"BI\", \"RW\", \"RJ\", \"3Q\",\n",
    "    \"RG\", \"L8\", \"L9\", \"7R\", \"5R\", \"LH\", \"CL\", \"LH\", \"VL\", \"LG\", \"NJ\", \"GJ\", \"LT\",\n",
    "    \"WB\", \"YL\", \"LN\", \"5U\", \"4P\", \"EX\", \"8N\", \"F2\", \"YX\", \"L2\", \"L0\", \"RI\", \"FC\",\n",
    "    \"UJ\", \"DI\", \"O8\", \"MP\", \"4M\", \"M7\", \"MY\", \"2M\", \"2Y\", \"N7\", \"C6\", \"LL\", \"5G\",\n",
    "    \"MJ\", \"9T\", \"W5\", \"6M\", \"AE\", \"7Y\", \"NR\", \"3W\", \"DB\", \"MH\", \"VM\", \"DJ\", \"4X\",\n",
    "    \"N5\", \"BM\", \"YV\", \"WD\", \"L6\", \"Q2\", \"MT\", \"XF\", \"ME\", \"M4\", \"UB\", \"8M\", \"K7\",\n",
    "    \"J4\", \"OD\", \"ID\", \"UP\", \"PG\", \"V9\", \"VK\", \"QH\", \"NB\", \"2T\", \"J8\", \"RR\", \"VS\",\n",
    "    \"VA\", \"XR\", \"BD\", \"J0\", \"8E\", \"CH\", \"3B\", \"JV\", \"JD\", \"VN\", \"0V\", \"V4\", \"B2\",\n",
    "    \"AB\", \"VI\", \"N3\", \"Q6\", \"Y4\", \"V7\", \"OB\", \"2L\", \"U4\", \"RP\", \"UZ\", \"VY\", \"B3\",\n",
    "    \"4B\", \"LB\", \"FB\", \"TF\", \"NT\", \"TT\", \"1X\", \"SN\", \"MX\", \"SI\", \"BV\", \"BZ\", \"BO\",\n",
    "    \"BZ\", \"BG\", \"VB\", \"BH\", \"UK\", \"V4\", \"VJ\", \"VU\", \"B4\", \"UQ\", \"NT\", \"VP\", \"OL\",\n",
    "    \"OL\", \"SV\", \"S1\", \"WN\", \"2S\", \"9R\", \"F2\", \"ZF\", \"FA\", \"SC\", \"OV\", \"SO\", \"FM\",\n",
    "    \"MF\", \"SH\", \"CE\", \"IH\", \"9X\", \"PL\", \"SY\", \"R8\", \"SR\", \"2U\", \"S6\", \"2R\", \"YH\",\n",
    "    \"EZ\", \"WG\", \"ZH\", \"DK\", \"ER\", \"D2\", \"5J\", \"DG\", \"C2\", \"K3\", \"PV\", \"IS\", \"9M\",\n",
    "    \"5Z\", \"SZ\", \"6J\", \"FW\", \"IE\", \"SP\", \"SK\", \"DR\", \"SD\", \"SJ\", \"PY\", \"Y8\", \"N9\",\n",
    "    \"IU\", \"UL\", \"5N\", \"6Y\", \"2N\", \"QS\", \"6D\", \"3Z\", \"7O\", \"2E\", \"LX\", \"WT\", \"Y3\",\n",
    "    \"ML\", \"3E\", \"BQ\", \"ZA\", \"GQ\", \"H2\", \"RD\", \"H8\", \"UY\", \"S8\", \"U3\", \"QU\", \"GG\",\n",
    "    \"BC\", \"PQ\", \"U5\", \"OW\", \"OO\", \"LC\", \"M8\", \"TE\", \"QN\", \"DO\", \"DV\", \"TR\", \"HK\",\n",
    "    \"S5\", \"2I\", \"JX\", \"4E\", \"4R\", \"7G\", \"SG\", \"SG\", \"P8\", \"NK\", \"C7\", \"YR\", \"5G\",\n",
    "    \"RB\", \"BB\", \"SI\", \"A2\", \"QG\", \"WX\", \"K5\", \"3M\", \"US\", \"7L\", \"ZP\", \"7E\", \"SQ\",\n",
    "    \"SQ\", \"XQ\", \"3U\", \"XO\", \"EH\", \"DM\", \"AG\", \"6A\", \"JI\", \"AR\", \"FG\", \"W3\", \"Z8\",\n",
    "    \"MZ\", \"A8\", \"M6\", \"AA\", \"8R\", \"XP\", \"YK\", \"4B\", \"ZR\", \"GU\", \"A0\", \"2K\", \"TA\",\n",
    "    \"WC\", \"QT\", \"LR\", \"AV\", \"9V\", \"X9\", \"X8\", \"JU\", \"4K\", \"KP\", \"0A\", \"8V\", \"P9\",\n",
    "    \"KP\", \"GM\", \"AM\", \"6R\", \"5D\", \"E4\", \"SU\", \"FW\", \"FI\", \"Q7\", \"F7\", \"2O\", \"I4\",\n",
    "    \"J2\", \"S4\", \"ZF\", \"AD\", \"2F\", \"AJ\", \"QP\", \"AK\", \"V8\", \"5Y\", \"RC\", \"J7\", \"AW\",\n",
    "    \"XU\", \"BU\", \"8U\", \"6L\", \"O4\", \"KO\", \"J5\", \"AS\", \"JN\", \"G4\", \"DQ\", \"6R\", \"KH\",\n",
    "    \"VC\", \"QQ\", \"UJ\", \"AP\", \"2B\", \"G0\", \"5A\", \"2G\", \"Q3\", \"A2\", \"IZ\", \"YE\", \"YC\",\n",
    "    \"R3\", \"AN\", \"4W\", \"9I\", \"UE\", \"A3\", \"WK\", \"B8\", \"EA\", \"ET\", \"EK\", \"BR\", \"8K\",\n",
    "    \"5V\", \"GD\", \"ES\", \"GT\", \"GL\", \"4N\", \"PX\", \"NZ\", \"EN\", \"HD\", \"RM\", \"GZ\", \"7I\",\n",
    "    \"LZ\", \"ZM\", \"MD\", \"NX\", \"KM\", \"MV\", \"MK\", \"4O\", \"NF\", \"KF\", \"BP\", \"2J\", \"C7\",\n",
    "    \"PJ\", \"HC\", \"JU\", \"HM\", \"Y2\", \"GI\", \"PF\", \"G9\", \"3O\", \"3L\", \"E5\", \"KC\", \"2A\",\n",
    "    \"NY\", \"ZB\", \"6I\", \"AH\", \"TZ\", \"CC\", \"UU\", \"3N\", \"N6\", \"ZW\", \"X5\", \"UX\", \"3H\",\n",
    "    \"AI\", \"IX\", \"9H\", \"CA\", \"CA\", \"DJ\", \"UM\", \"3C\", \"3E\", \"2Q\", \"TX\", \"TY\", \"AC\",\n",
    "    \"RV\", \"AC\", \"KS\", \"XK\", \"HF\", \"YN\", \"IK\", \"VT\", \"TN\", \"ST\", \"TC\", \"TS\", \"A6\",\n",
    "    \"8C\", \"6C\", \"8T\", \"7P\", \"FS\", \"AF\", \"F4\", \"P4\", \"HT\", \"LD\", \"AO\", \"TL\", \"KA\",\n",
    "    \"8G\", \"N2\", \"NL\", \"3S\", \"M0\", \"V5\", \"KW\", \"2S\", \"7L\", \"F5\", \"ZV\", \"XZ\", \"K2\",\n",
    "    \"EI\", \"EI\", \"4Z\", \"X8\", \"BT\", \"4Y\", \"RU\", \"PA\", \"T6\", \"AK\", \"D7\", \"ED\", \"NQ\",\n",
    "    \"JK\", \"EF\", \"SM\", \"SB\", \"P2\", \"RE\", \"ZD\", \"VF\", \"8J\", \"ZU\", \"ET\", \"EY\", \"EE\",\n",
    "    \"9E\", \"MQ\", \"E4\", \"G4\", \"7Q\", \"EL\", \"LY\", \"BS\", \"BA\", \"IY\", \"YT\", \"O7\", \"HZ\",\n",
    "    \"6O\", \"GR\", \"OC\", \"OG\", \"UI\", \"WY\", \"OF\", \"OS\", \"YI\", \"BK\", \"OA\", \"OY\", \"EB\",\n",
    "    \"WP\", \"Q9\", \"R5\", \"UR\", \"U6\", \"UQ\", \"HY\", \"PS\", \"4W\", \"OX\", \"WL\", \"3G\", \"2W\",\n",
    "    \"3P\", \"KD\", \"WU\", \"PN\", \"WS\", \"WR\", \"WC\", \"WF\", \"W6\", \"W4\", \"5W\", \"W9\", \"7W\",\n",
    "    \"WH\", \"P5\", \"IW\", \"UA\", \"B7\", \"U7\", \"6U\", \"QY\", \"H6\", \"PS\", \"YU\", \"Q4\", \"E6\",\n",
    "    \"EW\", \"YZ\", \"UF\", \"UT\", \"RF\", \"X7\", \"H7\", \"IA\", \"EP\", \"B9\", \"IR\", \"IO\", \"E9\",\n",
    "    \"I2\", \"YW\", \"IB\", \"QI\", \"E7\", \"T3\", \"I8\", \"MG\", \"RD\", \"7Z\", \"U2\", \"DS\", \"EC\",\n",
    "    \"MS\", \"EO\", \"E7\", \"XN\", \"7A\", \"QE\", \"QZ\", \"6E\", \"I7\", \"JY\", \"I4\", \"8B\", \"V8\",\n",
    "    \"IJ\", \"JL\", \"JC\", \"KZ\", \"JO\", \"J9\", \"JM\", \"ZN\", \"RY\", \"JG\", \"QK\", \"NU\", \"LC\",\n",
    "    \"NH\", \"JL\", \"ZK\", \"B6\", \"WJ\", \"JA\", \"JZ\", \"3K\", \"GK\", \"JQ\", \"4J\", \"LS\", \"JO\",\n",
    "    \"JR\", \"A9\", \"GH\", \"6J\", \"3J\", \"IM\", \"HO\", \"CZ\", \"CZ\", \"CO\", \"MU\", \"G5\", \"I9\",\n",
    "    \"CK\", \"CI\", \"XM\", \"D4\", \"ZG\", \"9D\", \"TZ\", \"KN\", \"CF\", \"VC\", \"6Q\", \"X7\", \"5C\",\n",
    "    \"GS\", \"HT\", \"EU\", \"GM\", \"9C\", \"OQ\", \"QW\", \"C3\", \"6L\", \"CV\", \"C8\", \"W8\", \"PM\",\n",
    "    \"HH\", \"8F\", \"NV\", \"V3\", \"BW\", \"QC\", \"Z7\", \"9Q\", \"IQ\", \"QR\", \"VR\", \"A7\", \"K4\",\n",
    "    \"K9\", \"MO\", \"RQ\", \"K6\", \"KR\", \"5T\", \"AU\", \"CX\", \"C5\", \"6C\", \"LF\", \"GY\", \"HQ\",\n",
    "    \"KQ\", \"QB\", \"8K\", \"KX\", \"9K\", \"M5\", \"4K\", \"CD\", \"XC\", \"XR\", \"SS\", \"KO\", \"GW\",\n",
    "    \"CQ\", \"7C\", \"CM\", \"P5\", \"FC\", \"DE\", \"V0\", \"C4\", \"8Z\", \"QF\", \"CU\", \"KU\", \"KY\",\n",
    "    \"QO\", \"QA\", \"KV\", \"C8\", \"OU\", \"VE\", \"KG\", \"FK\", \"Z3\", \"TB\", \"3T\", \"SF\", \"SL\",\n",
    "    \"VZ\", \"FD\", \"XJ\", \"TG\", \"IT\", \"ZT\", \"TM\", \"HJ\", \"K3\", \"TK\", \"TI\", \"TQ\", \"BV\",\n",
    "    \"T5\", \"T9\", \"U8\", \"TU\", \"UG\", \"M8\", \"T7\", \"8B\", \"HV\", \"TO\", \"R2\", \"Q8\", \"T7\",\n",
    "    \"C3\", \"TJ\", \"9N\", \"IL\", \"TD\", \"T7\", \"TV\", \"5P\", \"ZP\", \"P6\", \"FY\", \"PK\", \"HP\",\n",
    "    \"OP\", \"8Y\", \"P1\", \"8P\", \"BL\", \"PC\", \"7V\", \"FX\", \"FN\", \"UF\", \"DP\", \"PD\", \"NI\",\n",
    "    \"PO\", \"PI\", \"Z4\", \"FU\", \"6P\", \"BF\", \"P0\", \"F9\", \"FS\", \"FA\", \"4F\", \"E3\", \"3X\",\n",
    "    \"MI\", \"FH\", \"P6\", \"PW\", \"SX\", \"WV\", \"XY\", \"FL\", \"5M\", \"IF\", \"S9\", \"G6\", \"EQ\",\n",
    "    \"8W\", \"F0\", \"5F\", \"FT\", \"9P\", \"FP\", \"FZ\", \"FO\", \"FA\", \"F3\", \"3F\", \"F6\", \"YS\",\n",
    "    \"D3\", \"PU\", \"F8\", \"OG\", \"W2\", \"PT\", \"FJ\", \"MM\", \"PE\", \"AY\", \"HI\", \"Z2\", \"PR\",\n",
    "    \"YB\", \"HA\", \"5K\", \"3L\", \"HU\", \"HG\", \"H4\", \"H7\", \"HR\", \"NS\", \"HN\", \"H3\", \"H3\",\n",
    "    \"JB\", \"2L\", \"QX\", \"5Q\", \"HD\", \"UO\", \"HX\", \"RH\", \"RS\", \"WI\", \"JH\", \"MR\", \"HJ\",\n",
    "    \"H9\", \"OI\",\n",
    "\n",
    "    \"HL\" # 학습데이터에 있는 가짜 항공편 코드\n",
    "]"
   ],
   "id": "9dee68e53b542ba5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    KoBERT 기반 전처리에 적합하도록 특수문자 제거 및 공백 정리\n",
    "    \"\"\"\n",
    "    # 한글, 영문, 숫자, 공백만 남기기\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]\", \"\", str(text))\n",
    "    # 다중 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 플레이스홀더\n",
    "FLIGHT_PREFIX = \"FLIGHT\"   # 토큰은 ⟪FLIGHT0⟫, ⟪FLIGHT1⟫ ... 형태로 생성\n",
    "TERMINAL_PREFIX = \"TERMINAL\"  # 토큰은 ⟪TERMINAL0⟫, ⟪TERMINAL1⟫ ... 형태로 생성\n",
    "# 유효한 항공사 코드만 매칭하는 패턴 생성\n",
    "airline_codes_pattern = '|'.join(AIRLINE_CODES)\n",
    "\n",
    "# 일반적인 항공편 패턴 (공백 없음)\n",
    "flight_pattern_normal = re.compile(rf'\\b({airline_codes_pattern})\\s*[-]?\\s*(\\d{{1,4}})\\b', re.IGNORECASE)\n",
    "\n",
    "# 띄어쓰기된 항공편 패턴 (예: \"HL 7201\", \"7 C 0102\")\n",
    "flight_pattern_spaced = re.compile(r'\\b([A-Za-z0-9])\\s+([A-Za-z0-9])\\s+(\\d{1,4})\\b', re.IGNORECASE)\n",
    "\n",
    "def _collapse_flight_spans(text: str) -> str:\n",
    "    \"\"\"항공편 표현을 항상 붙여쓰기(하이픈/공백 제거) + 대문자로 통일.\"\"\"\n",
    "    # 일반 패턴 처리 (ke 907 -> KE907, KE 907 -> KE907)\n",
    "    text = flight_pattern_normal.sub(lambda m: (m.group(1) + m.group(2)).upper(), text)\n",
    "\n",
    "    # 띄어쓰기된 패턴 처리 (7 c 0102 -> 7C0102, hl 7201 -> HL7201)\n",
    "    def spaced_replacer(m):\n",
    "        code = m.group(1) + m.group(2)  # 항공사 코드 결합\n",
    "        number = m.group(3)             # 항공편 번호\n",
    "        # 유효한 항공사 코드인지 확인\n",
    "        if code.upper() in AIRLINE_CODES:\n",
    "            return (code + number).upper()  # 대문자로 변환\n",
    "        return m.group(0)  # 매칭되지 않으면 원본 유지\n",
    "\n",
    "    text = flight_pattern_spaced.sub(spaced_replacer, text)\n",
    "    return text\n",
    "\n",
    "def _collapse_terminal_spans(text: str) -> str:\n",
    "    \"\"\"터미널 표현을 T1, T2로 정규화.\"\"\"\n",
    "    # T1 관련 패턴들\n",
    "    t1_patterns = [\n",
    "        r'(?:제?\\s*1\\s*(?:여객\\s*)?터미널|터미널\\s*1|T\\s*-?\\s*1|첫\\s*번?\\s*째\\s*(?:여객\\s*)?터미널|제일\\s*(?:여객\\s*)?터미널)',\n",
    "        r'(?:일\\s*(?:여객\\s*)?터미널|터미널\\s*일)',\n",
    "        r'(?:제\\s*1\\s*여객\\s*터미널|제1\\s*여객\\s*터미널)',\n",
    "    ]\n",
    "\n",
    "    # T2 관련 패턴들\n",
    "    t2_patterns = [\n",
    "        r'(?:제?\\s*2\\s*(?:여객\\s*)?터미널|터미널\\s*2|T\\s*-?\\s*2|두\\s*번?\\s*째\\s*(?:여객\\s*)?터미널|제이\\s*(?:여객\\s*)?터미널)',\n",
    "        r'(?:이\\s*(?:여객\\s*)?터미널|터미널\\s*이)',\n",
    "        r'(?:제\\s*2\\s*여객\\s*터미널|제2\\s*여객\\s*터미널)',\n",
    "    ]\n",
    "\n",
    "    # T1으로 정규화\n",
    "    for pattern in t1_patterns:\n",
    "        text = re.sub(pattern, 'T1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # T2로 정규화\n",
    "    for pattern in t2_patterns:\n",
    "        text = re.sub(pattern, 'T2', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "_FACILITY_LIST = [\"기도실\", \"검역장\", \"수유실\"]  # 필요하면 여기에 계속 추가\n",
    "def _collapse_keyword(text: str, word: str) -> str:\n",
    "    base = word.replace(\" \", \"\")\n",
    "    # 한글/영문/숫자 경계에서만 매치되게 경계 추가\n",
    "    pattern = r'(?<![가-힣A-Za-z0-9])' + r'\\s*'.join(map(re.escape, base)) + r'(?![가-힣A-Za-z0-9])'\n",
    "    return re.sub(pattern, base, text)\n",
    "\n",
    "def _collapse_facility_spans(text: str) -> str:\n",
    "    for w in _FACILITY_LIST:\n",
    "        text = _collapse_keyword(text, w)\n",
    "    return text\n",
    "\n",
    "def normalize_with_morph(text: str) -> str:\n",
    "    # 0) 특수문자 제거 및 공백 정리\n",
    "    processed_text = clean_text(text)\n",
    "\n",
    "    # 1) 항공편을 먼저 붙여쓰기 정규화 (KE 907 -> KE907)\n",
    "    processed_text = _collapse_flight_spans(processed_text)\n",
    "\n",
    "    # 1.5) 터미널 표현 정규화 (1터미널 -> T1, 제2터미널 -> T2)\n",
    "    processed_text = _collapse_terminal_spans(processed_text)\n",
    "\n",
    "    # 2) 항공편을 플레이스홀더로 치환 (여러 개 지원)\n",
    "    flight_map = {}  # 예: {'⟪FLIGHT0⟫': 'KE907', '⟪FLIGHT1⟫': 'VS5501'}\n",
    "    flight_counter = 0\n",
    "    def _flight_repl(m):\n",
    "        nonlocal flight_counter\n",
    "        code = (m.group(1) + m.group(2)).upper()     # 붙여쓰기 + 대문자 변환\n",
    "        token = f'⟪{FLIGHT_PREFIX}{flight_counter}⟫'\n",
    "        flight_map[token] = code\n",
    "        flight_counter += 1\n",
    "        return token\n",
    "\n",
    "    processed_text = flight_pattern_normal.sub(_flight_repl, processed_text)\n",
    "\n",
    "    # 2.5) 터미널을 플레이스홀더로 치환\n",
    "    terminal_map = {}  # 예: {'⟪TERMINAL0⟫': 'T1', '⟪TERMINAL1⟫': 'T2'}\n",
    "    terminal_counter = 0\n",
    "    def _terminal_repl(m):\n",
    "        nonlocal terminal_counter\n",
    "        terminal_code = m.group(0)  # T1 또는 T2\n",
    "        token = f'⟪{TERMINAL_PREFIX}{terminal_counter}⟫'\n",
    "        terminal_map[token] = terminal_code\n",
    "        terminal_counter += 1\n",
    "        return token\n",
    "\n",
    "    # T1, T2 패턴을 플레이스홀더로 치환\n",
    "    terminal_pattern = re.compile(r'\\bT[12]\\b')\n",
    "    processed_text = terminal_pattern.sub(_terminal_repl, processed_text)\n",
    "\n",
    "\n",
    "    # 3) 형태소 분석 (정규화/어간화 끔)\n",
    "    tokens = okt.morphs(processed_text, norm=False, stem=False)\n",
    "\n",
    "    # 4) 다시 문자열로 합치기\n",
    "    text_after = \" \".join(tokens)\n",
    "\n",
    "    # 5) 플레이스홀더 복원\n",
    "    #    토크나이즈가 공백을 끼워넣어도(⟪ FLIGHT 0 ⟫) 정확히 복원되도록 처리\n",
    "    for token, code in flight_map.items():\n",
    "        core = token[1:-1]  # 'FLIGHT0'\n",
    "        m = re.match(r'(FLIGHT)(\\d+)$', core)\n",
    "        if m:\n",
    "            pat = re.compile(r'⟪\\s*' + m.group(1) + r'\\s*' + m.group(2) + r'\\s*⟫')\n",
    "            text_after = pat.sub(code, text_after)\n",
    "        # 혹시 그대로 남아있으면 직접 치환\n",
    "        text_after = text_after.replace(token, code)\n",
    "\n",
    "    # 터미널 플레이스홀더 복원\n",
    "    for token, code in terminal_map.items():\n",
    "        core = token[1:-1]  # 'TERMINAL0'\n",
    "        m = re.match(r'(TERMINAL)(\\d+)$', core)\n",
    "        if m:\n",
    "            pat = re.compile(r'⟪\\s*' + m.group(1) + r'\\s*' + m.group(2) + r'\\s*⟫')\n",
    "            text_after = pat.sub(code, text_after)\n",
    "        # 혹시 그대로 남아있으면 직접 치환\n",
    "        text_after = text_after.replace(token, code)\n",
    "\n",
    "    # 6) 혹시 남은 공백/하이픈 변형을 다시 한 번 정규화\n",
    "    text_after = _collapse_flight_spans(text_after)\n",
    "    text_after = _collapse_terminal_spans(text_after)\n",
    "\n",
    "    text_after = _collapse_facility_spans(text_after)\n",
    "\n",
    "    # 7) 공백 정리\n",
    "    text_after = re.sub(r'\\s+', ' ', text_after).strip()\n",
    "    return text_after\n"
   ],
   "id": "d3722fa4c058892e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax, sigmoid\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# 🔧 설정 및 경로\n",
    "INTENT2IDX_PATH = \"best_models/intent-bce-v1/intent2idx.pkl\"\n",
    "SLOT2IDX_PATH = \"best_models/intent-bce-v1/slot2idx.pkl\"\n",
    "MODEL_PATH = \"best_models/intent-bce-v1/best_model.pt\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 🏗️ 모델 클래스 (기존과 동일)\n",
    "class KoBERTIntentSlotModel(nn.Module):\n",
    "    def __init__(self, num_intents, num_slots):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        self.intent_classifier = nn.Linear(hidden_size, num_intents)\n",
    "        self.slot_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_slots)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        return intent_logits, slot_logits\n",
    "\n",
    "# ✅ 인덱스 맵 및 모델 로딩\n",
    "def load_model_and_mappings():\n",
    "    \"\"\"모델과 매핑 정보 로드\"\"\"\n",
    "    # 인덱스 맵 로딩\n",
    "    with open(INTENT2IDX_PATH, \"rb\") as f:\n",
    "        intent2idx = pickle.load(f)\n",
    "    with open(SLOT2IDX_PATH, \"rb\") as f:\n",
    "        slot2idx = pickle.load(f)\n",
    "\n",
    "    idx2intent = {v: k for k, v in intent2idx.items()}\n",
    "    idx2slot = {v: k for k, v in slot2idx.items()}\n",
    "\n",
    "    # 모델 로드\n",
    "    model = KoBERTIntentSlotModel(num_intents=len(intent2idx), num_slots=len(slot2idx))\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False)\n",
    "\n",
    "    return model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot\n",
    "\n",
    "# 🧱 토큰 → 단어 병합 + 슬롯 정렬\n",
    "def merge_tokens_and_slots(tokens, slot_ids, idx2slot):\n",
    "    \"\"\"토큰과 슬롯 병합\"\"\"\n",
    "    merged = []\n",
    "    word = ''\n",
    "    current_slot = ''\n",
    "\n",
    "    for token, slot_id in zip(tokens, slot_ids):\n",
    "        slot = idx2slot.get(slot_id, 'O')\n",
    "\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "\n",
    "        if token.startswith(\"▁\"):  # 새 단어 시작\n",
    "            if word:\n",
    "                merged.append((word, current_slot))\n",
    "            word = token[1:]\n",
    "            current_slot = slot\n",
    "        else:\n",
    "            word += token.replace(\"▁\", \"\")\n",
    "\n",
    "    if word:\n",
    "        merged.append((word, current_slot))\n",
    "\n",
    "    return merged\n",
    "\n",
    "# 🔮 BCEWithLogitsLoss 기반 예측 함수\n",
    "def predict_with_bce(text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                     threshold=0.5, top_k_intents=3, max_length=64):\n",
    "    \"\"\"\n",
    "    BCEWithLogitsLoss로 학습된 모델을 위한 예측 함수\n",
    "\n",
    "    Args:\n",
    "        text: 입력 텍스트\n",
    "        threshold: Intent 분류 임계값 (default: 0.5)\n",
    "        top_k_intents: 상위 K개 인텐트 반환 (default: 3)\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        intent_logits, slot_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Intent 예측 (Sigmoid 기반)\n",
    "        intent_probs = sigmoid(intent_logits)[0]  # [num_intents]\n",
    "\n",
    "        # 임계값 이상의 인텐트들 찾기\n",
    "        high_confidence_intents = []\n",
    "        for i, prob in enumerate(intent_probs):\n",
    "            if prob.item() >= threshold:\n",
    "                intent_name = idx2intent[i]\n",
    "                high_confidence_intents.append((intent_name, prob.item()))\n",
    "\n",
    "        # 확률 순으로 정렬\n",
    "        high_confidence_intents.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 만약 임계값 이상인 게 없다면 최고 확률 하나만\n",
    "        if not high_confidence_intents:\n",
    "            max_idx = torch.argmax(intent_probs).item()\n",
    "            max_prob = intent_probs[max_idx].item()\n",
    "            high_confidence_intents = [(idx2intent[max_idx], max_prob)]\n",
    "\n",
    "        # Top-K 인텐트 (전체 순위용)\n",
    "        topk_probs, topk_indices = torch.topk(intent_probs, min(top_k_intents, len(intent2idx)))\n",
    "        all_top_intents = [(idx2intent[idx.item()], prob.item())\n",
    "                          for idx, prob in zip(topk_indices, topk_probs)]\n",
    "\n",
    "        # 슬롯 예측 (기존과 동일 - Softmax 기반)\n",
    "        slot_pred_ids = torch.argmax(slot_logits, dim=2)[0].tolist()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        merged_slots = merge_tokens_and_slots(tokens, slot_pred_ids, idx2slot)\n",
    "\n",
    "    return {\n",
    "        'high_confidence_intents': high_confidence_intents,  # 임계값 이상\n",
    "        'all_top_intents': all_top_intents,                  # 전체 Top-K\n",
    "        'slots': merged_slots,\n",
    "        'is_multi_intent': len(high_confidence_intents) > 1,\n",
    "        'max_intent_prob': max(prob for _, prob in all_top_intents),\n",
    "        'intent_probs_raw': intent_probs.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# 🎯 라우팅 결정 함수 (3구간 임계값)\n",
    "def make_routing_decision(text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                         tau_hi=0.8, tau_lo=0.3, multi_threshold=0.5):\n",
    "    \"\"\"\n",
    "    3구간 임계값 기반 라우팅 결정\n",
    "\n",
    "    Args:\n",
    "        tau_hi: 높은 임계값 (바로 라우팅)\n",
    "        tau_lo: 낮은 임계값 (gray zone)\n",
    "        multi_threshold: 복합 의도 판단 임계값\n",
    "    \"\"\"\n",
    "    result = predict_with_bce(\n",
    "        text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "        threshold=multi_threshold\n",
    "    )\n",
    "\n",
    "    max_prob = result['max_intent_prob']\n",
    "    is_multi = result['is_multi_intent']\n",
    "\n",
    "    # 복합 의도인 경우\n",
    "    if is_multi:\n",
    "        decision = \"multi_intent\"\n",
    "        action = f\"🧠 메인 LLM 처리: 복합 의도 ({len(result['high_confidence_intents'])}개)\"\n",
    "        llm_type = \"main\"\n",
    "    # 단일 의도 + 높은 신뢰도\n",
    "    elif max_prob >= tau_hi:\n",
    "        decision = \"route\"\n",
    "        top_intent = result['all_top_intents'][0][0]\n",
    "        action = f\"✅ 직접 라우팅: {top_intent} 핸들러 호출\"\n",
    "        llm_type = None\n",
    "    # 단일 의도 + 낮은 신뢰도\n",
    "    else:\n",
    "        decision = \"abstain\"\n",
    "        action = \"🧠 메인 LLM 처리: 신뢰도 낮음, 전체 의도 분석 필요\"\n",
    "        llm_type = \"main\"\n",
    "\n",
    "    return {\n",
    "        'decision': decision,\n",
    "        'action': action,\n",
    "        'llm_type': llm_type,\n",
    "        'confidence': max_prob,\n",
    "        'intents': result['high_confidence_intents'],\n",
    "        'all_intents': result['all_top_intents'],\n",
    "        'slots': result['slots'],\n",
    "        'is_multi_intent': is_multi\n",
    "    }\n",
    "\n",
    "# 🔍 상세 분석 함수\n",
    "def analyze_prediction(text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                      threshold=0.5, show_all_probs=False):\n",
    "    \"\"\"상세한 예측 분석\"\"\"\n",
    "    result = predict_with_bce(\n",
    "        text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "        threshold=threshold\n",
    "    )\n",
    "\n",
    "    print(f\"\\n📝 입력: {text}\")\n",
    "    print(f\"🎯 임계값: {threshold}\")\n",
    "    print(f\"🔢 복합 의도 여부: {'Yes' if result['is_multi_intent'] else 'No'}\")\n",
    "\n",
    "    print(f\"\\n🏆 임계값 이상 인텐트 ({len(result['high_confidence_intents'])}개):\")\n",
    "    for i, (intent, prob) in enumerate(result['high_confidence_intents'], 1):\n",
    "        print(f\"   {i}. {intent}: {prob:.4f}\")\n",
    "\n",
    "    print(f\"\\n📊 전체 Top-{len(result['all_top_intents'])} 인텐트:\")\n",
    "    for i, (intent, prob) in enumerate(result['all_top_intents'], 1):\n",
    "        print(f\"   {i}. {intent}: {prob:.4f}\")\n",
    "\n",
    "    print(f\"\\n🎭 슬롯 태깅 결과:\")\n",
    "    for word, slot in result['slots']:\n",
    "        print(f\"   - {word}: {slot}\")\n",
    "\n",
    "    if result['is_multi_intent']:\n",
    "        print(f\"\\n🎯 복합 의도 감지됨!\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# 🧪 인터랙티브 테스트 함수\n",
    "def interactive_test():\n",
    "    \"\"\"인터랙티브 테스트\"\"\"\n",
    "    print(\"🚀 BCEWithLogitsLoss 기반 인텐트/슬롯 예측기\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 모델 로드\n",
    "    print(\"📥 모델 로딩 중...\")\n",
    "    model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot = load_model_and_mappings()\n",
    "    print(\"✅ 모델 로딩 완료!\")\n",
    "\n",
    "    print(f\"📊 인텐트 클래스: {len(intent2idx)}개\")\n",
    "    print(f\"📊 슬롯 클래스: {len(slot2idx)}개\")\n",
    "    print(\"\\n💡 사용법:\")\n",
    "    print(\"  - 텍스트 입력 시 예측 및 라우팅 결정 결과 표시\")\n",
    "    print(\"  - 임계값 변경: /threshold [값]\")\n",
    "    print(\"  - 종료: exit\")\n",
    "\n",
    "\n",
    "    threshold = 0.5 # Default threshold for analyze_prediction\n",
    "    multi_threshold = 0.5 # Default threshold for make_routing_decision\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"\\n✉️ 입력 (Analyze Thresh={threshold:.2f}, Multi Thresh={multi_threshold:.2f}): \").strip()\n",
    "\n",
    "        user_input = normalize_with_morph(user_input)\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"👋 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        if user_input.startswith(\"/threshold\"):\n",
    "            try:\n",
    "                parts = user_input.split()\n",
    "                if len(parts) > 1:\n",
    "                    new_threshold = float(parts[1])\n",
    "                    threshold = max(0.0, min(1.0, new_threshold))\n",
    "                    print(f\"🎯 상세 분석 임계값 변경: {threshold:.2f}\")\n",
    "                if len(parts) > 2:\n",
    "                    new_multi_threshold = float(parts[2])\n",
    "                    multi_threshold = max(0.0, min(1.0, new_multi_threshold))\n",
    "                    print(f\"🎯 복합 의도 임계값 변경: {multi_threshold:.2f}\")\n",
    "                elif len(parts) == 2:\n",
    "                    print(\"💡 복합 의도 임계값도 함께 변경하려면 `/threshold [분석 임계값] [복합 의도 임계값]` 형식으로 입력하세요.\")\n",
    "\n",
    "            except:\n",
    "                print(\"❌ 사용법: /threshold [분석 임계값] [복합 의도 임계값 (선택 사항)]\")\n",
    "            continue\n",
    "\n",
    "        # Process any input as a query\n",
    "        if user_input:\n",
    "            # Routing decision\n",
    "            routing_result = make_routing_decision(\n",
    "                user_input, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                multi_threshold=multi_threshold\n",
    "            )\n",
    "            print(f\"\\n--- 라우팅 결정 ---\")\n",
    "            print(f\"🎯 결정: {routing_result['decision'].upper()}\")\n",
    "            print(f\"📊 최대 신뢰도: {routing_result['confidence']:.4f}\")\n",
    "            print(f\"🔄 액션: {routing_result['action']}\")\n",
    "            if routing_result['intents']:\n",
    "                 intents_str = \", \".join([f\"{intent}({prob:.3f})\"\n",
    "                                          for intent, prob in routing_result['intents']])\n",
    "                 print(f\"🏷️ 예측 의도 (임계값 {multi_threshold:.2f} 이상): {intents_str}\")\n",
    "\n",
    "            # Detailed analysis\n",
    "            print(f\"\\n--- 상세 예측 분석 ---\")\n",
    "            analyze_prediction(\n",
    "                user_input, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                threshold=threshold, show_all_probs=False # show_all_probs는 항상 False로 유지\n",
    "            )\n",
    "\n",
    "\n",
    "# 🎮 간단한 예측 함수 (기존 스타일 호환)\n",
    "def predict_top_k_intents_and_slots(text, k=3, threshold=0.5):\n",
    "    \"\"\"기존 스타일과 호환되는 간단한 예측 함수\"\"\"\n",
    "    model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot = load_model_and_mappings()\n",
    "\n",
    "    result = predict_with_bce(\n",
    "        text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "        threshold=threshold, top_k_intents=k\n",
    "    )\n",
    "\n",
    "    # 기존 형식으로 반환\n",
    "    intents = result['all_top_intents']\n",
    "    slots = result['slots']\n",
    "\n",
    "    return intents, slots\n",
    "\n",
    "\n",
    "# 🚀 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 인터랙티브 모드\n",
    "    interactive_test()\n",
    "\n",
    "    # 또는 간단한 테스트\n",
    "    # intents, slots = predict_top_k_intents_and_slots(\"내일 비행기 시간표 알려주세요\")\n",
    "    # print(\"인텐트:\", intents)\n",
    "    # print(\"슬롯:\", slots)"
   ],
   "id": "b659ef41ae74cfdf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e79bcb1a243d238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa38e95496c3e4ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16b02c3eb4734cd6"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
